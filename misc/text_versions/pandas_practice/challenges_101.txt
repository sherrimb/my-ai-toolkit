File path: ./pandas_practice/challenges_101.ipynb

# **Exploring Pandas: Common Data Operations**

Welcome to this Jupyter Notebook! ðŸš€ In this notebook, you'll practice some of the most commonly used operations in the **Pandas** library using **two datasets**:
1. **students.csv** (CSV)
2. **enrollments.json** (JSON)

These files should be placed in the same folder as this notebook. By the end, you'll have a strong grasp of common data manipulation tasks, and you'll even merge these two datasets on a common key.

Before starting, make sure you have **Pandas** installed. (It should come preinstalled in Anaconda!)

If pandas is not installed, follow the instructions below.

---

## **Checking if Pandas is Installed in Your Conda Environment**

Before proceeding, check if Pandas is installed in your Conda environment by running the following command in a **Jupyter Notebook** cell:


```python
import pandas as pd
print(pd.__version__)
```

    2.2.2


If this runs without errors and prints a version number, Pandas is installed. If you see an **ImportError**, install Pandas using one of the following methods:

### **For Conda Users (Recommended)**
Run this in your terminal or Anaconda Prompt:
```
conda install pandas
```

### **Using Conda-Forge (If Needed)**
If you encounter issues, you can install Pandas from **Conda-Forge**, a community-maintained repository with up-to-date packages:
```
conda install -c conda-forge pandas
```

### **For Pip Users**
If you're using a virtual environment outside Conda, install Pandas via Pip:
```
pip install pandas
```

# Now, letâ€™s dive in! ðŸŠâ€â™‚ï¸

---


## **1. Load a CSV file into a Pandas DataFrame**

First, let's **import Pandas** and load the datasets. Two datasets have been prepared for you:

- `students.csv`
- `enrollments.json`

You will use these two datasets for the following challenges.

**ðŸ’¡ Hint:** If the file is in the same directory as your notebook, you can just use the filename. Otherwise, provide the full file path.



```python
# your code here
```


```python
# your code here
```

## **2. View the First and Last Few Rows of Each DataFrame**

Check out how your data looks. One method previews the first few records, while another method previews the last few. You can specify the number of rows you want to see by explicitly passing an integer argument.

**ðŸ“ Tip:** This is a great time to confirm that columns loaded correctly and to spot any obvious data issues (strange values, mismatched columns, etc.).


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **3. Check the Shape of Each DataFrame**

To understand the **size** of your dataset(s), use the attribute that returns `(number_of_rows, number_of_columns)`.

**ðŸ“ Tip:** Note any big differences in row counts that might affect merging later.



```python
# your code here

```


```python
# your code here
```

## **4. Get a Summary of Each DataFrame**

Explore one or two approaches that provide:

-   **Column names**
-   **Data types**
-   **Basic statistics about numerical columns**
-   **Number of non-null values**

**ðŸ“ Tip:** One approach might give an overview of columns and data types; another might summarize numerical columns. This step helps you detect columns that might need cleaning.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **5. Check for Missing Values**

Determine if your dataset has any missing or null values by **counting** them. Notice which columns have many missing entries and plan how to handle them.

**ðŸ“ Tip:** Some columns might look present but contain empty strings. Identify them if possible.



```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **6. Rename Columns for Clarity and Consistency**

Some columns may have **spaces** or **capitalization** that complicates your analysis. For example, if you see `"current gpa"` or `"First_Name"`, consider renaming them (e.g., `"current_gpa"`, `"first_name"`) for ease of use.

**ðŸ“ Tip:** Consistent naming conventions help minimize typos and KeyErrors.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **7. Convert Data Types Where Needed**

Check which columns should be numeric or datetime. Columns like `admission_year` or `course_fee` might be read as **strings** by default. Convert them to numerical or date formats if necessary.

**ðŸ“ Tip:** Make sure you handle errors gracefully (e.g., set `errors='coerce'` to turn invalid entries into NaN).


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **8. Fill Missing Values with a Specified Value or Method**

Instead of **dropping** missing values, consider **replacing** them. For instance:

-   A string like `"Unknown"` for missing text
-   A **mean** or **median** for missing numeric columns
-   A **forward** or **backward fill** if appropriate


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **9. Drop Rows or Columns with Missing Values (If Needed)**

After considering which values can be filled, you might choose to **remove** rows or columns that are missing too much data or canâ€™t be fixed.

**ðŸ“ Tip:** Decide carefully and confirm you donâ€™t need the dropped information. Use `inplace=True` or keep a separate DataFrame if you want to preserve the original data.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **10. Filter Rows Based on a Condition**

Now that columns like `admission_year` and `course_fee` (or `current_gpa`) are numeric, experiment with filtering. For example:

-   Students whose `admission_year` is after a certain date
-   Enrollments for `Spring 2026`


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **11. Select Specific Columns from Each DataFrame**

Often, you donâ€™t need all columns at once. For instance, you might extract only:

-   `"student_id"`, `"First_Name"`, `"last_name"`, and `"current_gpa"` from `students.csv`
-   `"stud_ref_id"`, `"course_title"`, `"instructor_name"`, `"course_fee"` from `enrollments.json`


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **12. Sort the DataFrame by One or More Columns**

Sorting can help you identify which records have the highest or lowest values. For example:

-   Sort the **students** DataFrame by `"current_gpa"` in descending order
-   Sort the **enrollments** DataFrame by `"course_fee"` in ascending order

**ðŸ“ Tip:** You can sort by multiple columns if needed.



```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **13. Group Data by a Column and Compute Aggregate Functions**

Grouping lets you see aggregated info by category. For example, group **students** by `"majorField"` and compute the average `"current_gpa"`. In **enrollments**, group by `"instructor_name"` and compute the average `"course_fee"`.

**ðŸ“ Tip:** Aggregations might include `.mean()`, `.sum()`, `.count()`, etc.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **14. Apply a Custom Function**

Define a normal Python function to transform data in a column. For example, title-case a name or uppercase a field. Apply that function to each element in the column.

**ðŸ“ Tip:** If your function references another library call or has complex logic, define it above and then use `.apply(...)` with your function name. Once you've done this, see if you do this using lamda notation. 


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **15. Create a New Column Based on Existing Ones**

Use existing columns to generate new ones. For instance, combine `"First_Name"` and `"last_name"` into `"full_name"`, or compute `"fees_after_tax"` in enrollments if you assume a tax rate.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **16. Merge Two DataFrames on a Common Column**

Combine `students.csv` and `enrollments.json` by matching:

-   `stu["student_id"]`
-   `enr["stud_ref_id"]` (or rename it first)

Check the shape of the merged DataFrame afterward to ensure it merged as expected.



```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **17. Remove Duplicate Rows**

When merging or concatenating multiple files, duplicates can crop up. Identify them and remove if needed. This might be especially important if the same student or enrollment is listed more than once.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **18. Additional Data Cleaning**

Now that youâ€™ve merged or manipulated your data, do a quick final pass:

-   Fix any remaining oddities (e.g., negative phone numbers or impossible dates)
-   Normalize columns further (e.g., standardize text formatting)

**ðŸ“ Tip:** You might revisit previous steps if new issues appear.



```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **19. Save the Cleaned and Merged DataFrame to a New CSV File**

Finally, when youâ€™re satisfied with your cleaned data, save it. Remember to avoid writing the index as a separate column unless you want it.


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

## **20. Explore Further Analyses (Optional)**

Now that your data is in great shape, try some optional challenges:

-   Generate charts or visualizations
-   Perform advanced filtering or grouping
-   Create pivot tables
-   Or anything else that interests you!


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```


```python
# your code here
```

**ðŸŽ‰ Congratulations!** Youâ€™ve now tackled **data cleaning** and many essential **Pandas** operations in `students.csv` and `enrollments.json`. Keep experimenting to sharpen your **data manipulation skills** and unlock deeper insights! ðŸ’ª

# **Example Solutions**

Below is a **revised solutions section** that demonstrates **practical outputs** in each step, handles **admission_year** as an integer, and shows **three merge alternatives** (inner, left, outer). We also show some **data cleaning** for special cases (e.g., the string `"twenty twenty two"`). Adjust column names and file paths as needed to match your exact data.

---

## **1. Load a CSV File into a Pandas DataFrame**

**Discussion:**  
Weâ€™ll import **Pandas** and load two files:
- **students.csv** â†’ `stu`
- **enrollments.json** â†’ `enr`

**We also** display the **shape** of the DataFrames to ensure that they loaded correctly.

```python
import pandas as pd

# 1) Load students.csv and enrollments.json
stu = pd.read_csv("students.csv")
enr = pd.read_json("enrollments.json")

# 2) Show shape and a brief peek to confirm loading
print("Students shape:", stu.shape)
print("Enrollments shape:", enr.shape)

print("\n=== Students (first 3 rows) ===")
display(stu.head(3))

print("\n=== Enrollments (first 3 rows) ===")
display(enr.head(3))
```

**Insight:**  
Think of **students.csv** like a "directory" of people, and **enrollments.json** like "course registrations" for those people. Each dataset focuses on a different dimension.

---

## **2. View the First and Last Few Rows of Each DataFrame**

**Discussion:**  
- `.head()` reveals the first few rows (default 5).
- `.tail()` reveals the last few rows.
- Great for a quick sanity check.

```python
print("=== Students: First 2 ===")
display(stu.head(2))

print("\n=== Students: Last 2 ===")
display(stu.tail(2))

print("\n=== Enrollments: First 2 ===")
display(enr.head(2))

print("\n=== Enrollments: Last 2 ===")
display(enr.tail(2))
```

**Tip:**  
".head()" and ".tail()" are like flipping through the first and last pages of a big ledger to see if data is correct.

---

## **3. Check the Shape of Each DataFrame**

**Discussion:**  
- `.shape` returns `(rows, columns)`.
- We might want to ensure `stu` and `enr` have the rows/columns we expect.

```python
print("Students shape:", stu.shape)
print("Enrollments shape:", enr.shape)
```

---

## **4. Get a Summary of Each DataFrame**

**Discussion:**  
1. `.info()` â†’ column names, data types, non-null counts.  
2. `.describe()` â†’ descriptive stats for numeric columns.

```python
# Show column info
print("=== Students Info ===")
stu.info()
print("\n=== Enrollments Info ===")
enr.info()

# Show numeric stats
print("\n=== Students Describe ===")
display(stu.describe(include='all'))  # 'all' to see object columns if you wish
print("\n=== Enrollments Describe ===")
display(enr.describe(include='all'))
```

**Insight:**  
- If you see `object` for a numeric column, that signals a need for type conversion (see Challenge 7).  
- Use `.describe(include='all')` to see summary of non-numeric columns.

---

## **5. Check for Missing Values**

**Discussion:**  
- `.isnull().sum()` shows how many nulls per column.  
- Good for planning how to handle missing data.

```python
print("=== Missing in Students ===")
print(stu.isnull().sum())

print("\n=== Missing in Enrollments ===")
print(enr.isnull().sum())
```

**Tip:**  
- You might also want to check for empty strings. For example:
  ```python
  (stu["first_name"] == "").sum()
  ```

---

## **6. Rename Columns for Clarity and Consistency**

**Discussion:**  
- If you see columns like `"First_Name"` or `"current gpa"`, rename them to `"first_name"` or `"current_gpa"`.  
- This helps avoid typos, KeyErrors, and confusion.

```python
stu = stu.rename(columns={
    "First_Name": "first_name",
    "last_name": "last_name",
    "Birthddate": "birthdate",
    "majorField": "major",
    "current gpa": "current_gpa",
    "mobile number": "mobile_number",
    "HOME COUNTRY": "home_country"
})

enr = enr.rename(columns={
    "stud_ref_id": "student_id"
})

print("\nRenamed columns in 'stu':", stu.columns.tolist())
print("Renamed columns in 'enr':", enr.columns.tolist())
```

**Analogy:**  
Renaming columns is like labeling boxes with consistent names so you can always find what you need.

---

## **7. Convert Data Types Where Needed**

**Discussion:**  
- Convert numeric columns from strings using `pd.to_numeric`.
- Convert date columns using `pd.to_datetime`.
- If some values are invalid (like `"twenty twenty two"`), `errors='coerce'` will produce `NaN`. We can fix them next.

```python
# Convert admission_year to numeric
stu["admission_year"] = pd.to_numeric(stu["admission_year"], errors="coerce")

# For any that remain NaN, let's fill them with 2022 if we suspect they're actually 2022 or use a default
stu["admission_year"] = stu["admission_year"].fillna(2022)

# If we want admission_year to be an integer (not float):
stu["admission_year"] = stu["admission_year"].astype(int)

# Convert current_gpa to numeric
stu["current_gpa"] = pd.to_numeric(stu["current_gpa"], errors="coerce")

# Convert birthdate to datetime
stu["birthdate"] = pd.to_datetime(stu["birthdate"], errors="coerce")

# Convert enrollments
enr["course_fee"] = pd.to_numeric(enr["course_fee"], errors="coerce")
enr["date_enrolled"] = pd.to_datetime(enr["date_enrolled"], errors="coerce")

print("\nUnique admission_year values:", stu["admission_year"].unique())
print("Unique current_gpa values (first 10):", stu["current_gpa"].unique()[:10])
```

**Insight:**  
- If a column like `admission_year` might have truly unknown values, you can fill them with 0 or another sentinel (e.g., 9999).  
- By converting to `int`, you guarantee no decimal points.

---

## **8. Fill Missing Values with a Specified Value or Method**

**Discussion:**  
- Instead of dropping rows with missing data, we can fill them.  
- For instance, `major` could become `"Undeclared"`, `current_gpa` might use the mean, etc.

```python
# Fill missing major with 'Undeclared'
stu["major"] = stu["major"].fillna("Undeclared")

# If current_gpa is missing, fill with the average
gpa_mean = stu["current_gpa"].mean()
stu["current_gpa"] = stu["current_gpa"].fillna(gpa_mean)

# For enrollments, fill missing course_fee with 0
enr["course_fee"] = enr["course_fee"].fillna(0)
```

**Tip:**  
- For text columns, `"Unknown"` is common. For numeric, consider mean, median, or domain-specific values.

---

## **9. Drop Rows or Columns with Missing Values (If Needed)**

**Discussion:**  
- After some fills, you might still have columns or rows with unresolvable missing data.  
- `.dropna()` can remove them. Choose rows (`axis=0`) or columns (`axis=1`).

```python
# Example: Drop rows in stu if birthdate is still missing
stu = stu.dropna(subset=["birthdate"])
print("After dropping missing birthdates, shape:", stu.shape)

# Example: Drop columns in enr if we still have large missing data
enr = enr.dropna(axis=1, how='all')
print("After dropping entirely-empty columns in enr, shape:", enr.shape)
```

**Insight:**  
- Double-check `.shape` so you donâ€™t remove too many rows or columns accidentally.

---

## **10. Filter Rows Based on a Condition**

**Discussion:**  
- Now that columns like `admission_year` or `course_fee` are numeric, we can do numeric filtering.  
- You might also filter by string (e.g., `term_offered == "Spring 2026"`).

```python
# Students admitted after 2020
stu_newest = stu[stu["admission_year"] > 2020]
print("Students admitted after 2020:", stu_newest.shape[0])

# Enrollments for 'Spring 2026'
enr_spring = enr[enr["term_offered"] == "Spring 2026"]
print("Spring 2026 enrollments:", enr_spring.shape[0])
```

**Tip:**  
- Combine conditions with `&`, `|`:
  ```python
  stu[(stu["admission_year"] > 2020) & (stu["gender"] == "F")]
  ```

---

## **11. Select Specific Columns from Each DataFrame**

**Discussion:**  
- Often you only want a subset for clarity.  
- E.g., keep `student_id`, `first_name`, `last_name`, `current_gpa` from students.

```python
stu_subset = stu[["student_id", "first_name", "last_name", "admission_year", "current_gpa", "major"]]
enr_subset = enr[["student_id", "subject_code", "course_title", "instructor_name", "course_fee", "term_offered"]]

print("\n=== Students subset (first 2 rows) ===")
display(stu_subset.head(2))

print("\n=== Enrollments subset (first 2 rows) ===")
display(enr_subset.head(2))
```

**Analogy:**  
- Like choosing which columns to display in a final report.

---

## **12. Sort the DataFrame by One or More Columns**

**Discussion:**  
- Sorting helps quickly identify highest or lowest values.  
- For multiple columns, pass a list.

```python
# Sort students by current_gpa descending
stu_sorted = stu.sort_values("current_gpa", ascending=False)
print("Top 3 by current_gpa:")
display(stu_sorted.head(3))

# Sort enrollments by course_fee ascending
enr_sorted = enr.sort_values("course_fee")
print("First 3 by course_fee:")
display(enr_sorted.head(3))
```

---

## **13. Group Data by a Column and Compute Aggregate Functions**

**Discussion:**  
- `.groupby()` helps you see aggregated values.  
- E.g., average `current_gpa` by `major`, average `course_fee` by `subject_code`.

```python
gpa_by_major = stu.groupby("major")["current_gpa"].mean()
fee_by_subject = enr.groupby("subject_code")["course_fee"].mean()

print("=== Average GPA by Major ===")
display(gpa_by_major)

print("\n=== Average Fee by Subject Code ===")
display(fee_by_subject)
```

**Insight:**  
- Add `.agg(["mean", "count", "max"])` for more stats in one line.

---

## **14. Apply a Custom Function**

**Discussion:**  
- You can define a Python function to fix or standardize text, then `.apply()` it across a Series (column).  
- Great for normalizing inconsistent entries, e.g., `'dr. smith'`, `'Dr. Smith'`.

```python
def titlecase_instructor(name):
    if pd.isnull(name):
        return name
    return str(name).title()

enr["instructor_name"] = enr["instructor_name"].apply(titlecase_instructor)

# Another example: uppercase last_name in stu
def uppercase_lastname(name):
    if pd.isnull(name):
        return name
    return str(name).upper()

stu["last_name"] = stu["last_name"].apply(uppercase_lastname)

print("After applying custom funcs:")
display(stu.head(2))
display(enr.head(2))
```

**Tip:**  
- If you prefer inline lambdas:
  ```python
  enr["instructor_name"] = enr["instructor_name"].apply(lambda x: str(x).title() if x else x)
  ```

---

## **15. Create a New Column Based on Existing Ones**

**Discussion:**  
- Combine fields (e.g., `first_name + last_name` â†’ `full_name`).
- Do arithmetic (e.g., `course_fee * 1.08` â†’ `fees_after_tax`).

```python
stu["full_name"] = stu["first_name"] + " " + stu["last_name"]
enr["fees_after_tax"] = enr["course_fee"] * 1.08

print("\n=== Students with new 'full_name' ===")
display(stu[["student_id", "full_name"]].head(3))

print("\n=== Enrollments with new 'fees_after_tax' ===")
display(enr[["student_id", "course_title", "course_fee", "fees_after_tax"]].head(3))
```

**Analogy:**  
- Similar to an Excel formula that references other columns.

---

## **16. Merge Two DataFrames on a Common Column**

**Discussion:**  
- Typically, the join key is `student_id`.  
- Because a single student can appear multiple times in `enr`, you get **one-to-many** rows in the result.

### **Option A: Inner Merge**
Only matches rows present in **both** DataFrames.  

```python
merged_inner = stu.merge(enr, on="student_id", how="inner")
print("\n[Inner Merge] shape:", merged_inner.shape)
display(merged_inner.head(2))
```

### **Option B: Left Merge**
Keeps **all students** from `stu`, even if they have no matching enrollment. Missing enrollments appear as NaN.  

```python
merged_left = stu.merge(enr, on="student_id", how="left")
print("\n[Left Merge] shape:", merged_left.shape)
display(merged_left.head(2))
```

### **Option C: Outer Merge**
Keeps **all rows** from **both** DataFrames. Students not found in `enr` or vice versa become NaN in the unmatched columns.  

```python
merged_outer = stu.merge(enr, on="student_id", how="outer")
print("\n[Outer Merge] shape:", merged_outer.shape)
display(merged_outer.head(2))
```

**Tip:**  
- Use `how="right"` if you want all enrollment rows, even if the student isnâ€™t in `stu`.  
- Typically, universities use a left or inner join. Outer is less common but can help find mismatches.

---

## **17. Remove Duplicate Rows**

**Discussion:**  
- In real data, you might see exact duplicates or accidental double-entries.  
- `.drop_duplicates()` keeps the first occurrence.

```python
merged_df = merged_inner.copy()  # We'll keep the 'inner' result for final usage
deduped_df = merged_df.drop_duplicates()
print("Before duplicates removal:", merged_df.shape, "After:", deduped_df.shape)
```

**Tip:**  
- If your definition of "duplicate" is specific (e.g., same student_id AND same course_title), use `subset=["student_id", "course_title"]`.

---

## **18. Additional Data Cleaning**

**Discussion:**  
- E.g., fix negative phone numbers or remove erroneous subject codes (`"HIST999"`).  
- We do these final touches to polish the dataset.

```python
# Example: fix negative phone numbers in 'stu'
stu["mobile_number"] = stu["mobile_number"].apply(lambda x: abs(x) if pd.notnull(x) else x)

# Example: remove spurious enrollment code "HIST999"
deduped_df = deduped_df[deduped_df["subject_code"] != "HIST999"]

print("Final deduped shape:", deduped_df.shape)
display(deduped_df.head(2))
```

**Analogy:**  
- Think of this like the final polishing or proofread before publishing a report.

---

## **19. Save the Cleaned and Merged DataFrame to a New CSV File**

**Discussion:**  
- Once satisfied, export. `index=False` avoids an extra "index" column in your CSV.

```python
deduped_df.to_csv("cleaned_students_enrollments.csv", index=False)
print("Saved final data to 'cleaned_students_enrollments.csv'.")
```

**Tip:**  
- You can also do `to_excel("mydata.xlsx")` or `to_json("mydata.json")` if desired.

---

## **20. Explore Further Analyses (Optional)**

**Discussion:**  
- With a clean, merged dataset, you can do advanced tasks:
  - Visualizations (e.g., `matplotlib`, `seaborn`)
  - Pivot tables
  - Machine learning

```python
# Quick example: histogram of admission_year
stu["admission_year"].hist(bins=10)
```

**Insight:**  
- This is where the real analysis beginsâ€”our data is now consistent and well-organized!

---

## **Final Thoughts**

1. **Handling Admission Year as Integer**  
   - We used `pd.to_numeric(..., errors='coerce')` followed by `.fillna(...)` and `.astype(int)` so `admission_year` is properly an integer.

2. **Merging Strategies**  
   - We demonstrated **inner**, **left**, and **outer** merges.  
   - **Inner** â†’ only matches present in both.  
   - **Left** â†’ all students, match enrollments where possible.  
   - **Outer** â†’ keep everything from both sides, even if unmatched.

3. **Practical Output**  
   - We used `display(...)` and `print(...)` to show shapes and head results. This helps confirm we achieved the **desired** transformations.

4. **Continue Testing**  
   - Always keep verifying with `.shape`, `.head()`, and `.isnull().sum()` after changes.  
   - Domain knowledge helps finalize decisions about missing data, duplicates, or outlier removal.

Enjoy your journey with **Pandas**! 



```python

```


```python

```
